import requests
from bs4 import BeautifulSoup
import csv
from urllib.parse import urljoin, urlparse
import re

# Set up parameters
base_url = "https://www.website.com" // https://www.website.com/this-directory-only/
visited_urls = set()
max_pages = 999999999  # Set max pages to crawl
page_count = 0
user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 (compatible; Screaming Frog)"
#"Center",
search_phrases = [
"Accessorize",
"Acclimatize",
"Accustomize",
"Actualize",
"Actualize",
"Adventurize",
"Advertize",
"Aerate",
"Aggrandize",
"Agonize",
"Alkalize",
"Alphabetize",
"Alphabetize",
"Ameba",
"Ameliorize",
"Americanize",
"Amortize",
"Analyze",
"Anatomize",
"Anemia",
"Anesthetic",
"Anglicize",
"Antagonize",
"Apologize",
"Appetize",
"Apprehensivize",
"Arbitrize",
"Archeology",
"Ardor",
"Armor",
"Authoritativize",
"Authorize",
"Autoanalyze",
"Automatize",
"Baptize",
"Behavior",
"Behavor",
"Bipolarize",
"Bowdlerize",
"Briticize",
"Caliber",
"Canceled",
"Candor",
"Cannibalize",
"Capitalize",
"Carbonize",
"Categorize",
"Categorize",
"Centralize",
"Centralize",
"Characterize",
"Circularize",
"Civilizationize",
"Civilize",
"Clamor",
"Colonialize",
"Colonize",
"Color",
"Communize",
"Compromise",
"Computerize",
"Conceptualize",
"Conceptualize",
"Concretize",
"Confederalize",
"Conjecturalize",
"Conjurize",
"Counter-mobilize",
"Credentialize",
"Crystallize",
"Decarburize",
"Decentralize",
"Decentralize",
"Decriminalize",
"Deculturize",
"Defense",
"Defense",
"Dehumanize",
"Democratize",
"Demonize",
"Deoxidize",
"Deregularize",
"Detribalize",
"Diarrhea",
"Digitize",
"Dolor",
"Dramatize",
"Economize",
"Editorialize",
"Electrize",
"Emphasize",
"Encyclopedia",
"Endeavor",
"Energize",
"Enrollment",
"Equalize",
"Estrogen",
"Etiology",
"Eulogize",
"Evangelize",
"Exaggerize",
"Familiarize",
"Familiarize",
"Favor",
"Fertilize",
"Fertilize",
"Fervor",
"Fiber",
"Finalize",
"Finalize",
"Flavor",
"Formalize",
"Formalize",
"Fossilize",
"Fraternize",
"Fulfill",
"Generalize",
"Generalize",
"Ghettoize",
"Glamor",
"Glamorize",
"Globalize",
"Gorgonize",
"Gouvernor",
"Gynecology",
"Harbor",
"Harmonize",
"Harmonize",
"Hellenize",
"Hellenize",
"Hematize",
"Hemispherize",
"Hemoglobin",
"Hemorrhage",
"Homogenize",
"Honor",
"Hospitalize",
"Humanize",
"Humor",
"Hypnotize",
"Hypothesize",
"Idealize",
"Idealize",
"Idolize",
"Immobilize",
"Immortalize",
"Immunize",
"Immunize",
"Imperialize",
"Imperialize",
"Individualize",
"Individualize",
"Industrialize",
"Industrialize",
"Installment",
"Intellectualize",
"Internalize",
"Internalize",
"Ionize",
"Itemize",
"Itemize",
"Jeopardize",
"Judaize",
"Kidnapped",
"Labeled",
"Labor",
"Lateralize",
"Legalize",
"Legalize",
"Leukemia",
"Liberalize",
"Liberalize",
"License",
"Liter",
"Lobotomize",
"Localize",
"Localize",
"Luster",
"Magnetize",
"Magnetize",
"Maneuver",
"Materialize",
"Materialize",
"Maximize",
"Mechanize",
"Medieval",
"Memorialize",
"Memorize",
"Mesmerize",
"Metalize",
"Meter",
"Meterage",
"Militarize",
"Miniaturize",
"Mistranslate",
"Mobilize",
"Mobilize",
"Modeling",
"Modernize",
"Modernize",
"Monopolize",
"Moralize",
"Mythologize",
"Nationalize",
"Nationalize",
"Naturalize",
"Neighbor",
"Neutralize",
"Neutralize",
"Nitrometer",
"Normalize",
"Normalize",
"Obligatize",
"Odor",
"Offense",
"Offense",
"Optimize",
"Organize",
"Organize",
"Orthopedic",
"Ostracize",
"Overemphasize",
"Oxidize",
"Oxidize",
"Paleography",
"Paleolithic",
"Paleontology",
"Paneling",
"Paralyze",
"Pasteurize",
"Patronize",
"Pediatric",
"Pediatrician",
"Penalize",
"Personalize",
"Personalize",
"Philosophize",
"Pluralize",
"Polarize",
"Polarize",
"Politicize",
"Popularize",
"Popularize",
"Pressurize",
"Pretense",
"Prioritize",
"Prioritize",
"Privatize",
"Privatize",
"Professionalize",
"Propagandize",
"Prophecize",
"Publicize",
"Pulverize",
"Pulverize",
"Quarreled",
"Rationalize",
"Rationalize",
"Realize",
"Realize",
"Recognize",
"Recognize",
"Regularize",
"Reorganize",
"Reorganize",
"Revolutionize",
"Revolutionize",
"Rigor",
"Romanticize",
"Rumor",
"Satirize",
"Satirize",
"Savior",
"Scepter",
"Scrutinize",
"Scrutinize",
"Sensitize",
"Sepulcher",
"Signalize",
"Skillful",
"Socialize",
"Socialize",
"Somber",
"Specialize",
"Specter",
"Splendor",
"Stabilize",
"Stabilize",
"Standardize",
"Standardize",
"Sterilize",
"Summarize",
"Summarize",
"Symbolize",
"Symbolize",
"Sympathize",
"Systematize",
"Systematize",
"Tantalize",
"Temporize",
"Terrorize",
"Theater",
"Theorize",
"Theorize",
"Tranquilize",
"Tranquilize",
"Traveling",
"Tumor",
"Tyrannize",
"Urbanize",
"Utilize",
"Utilize",
"Vandalize",
"Vaporize",
"Vaporize",
"Victimize",
"Vigor",
"Visualize",
"Visualize",
"Vocalize",
"Vocalize",
"Worshiped",
"Dialed",
"Signaled",
"Totaled",
"Canceled",
"Fueled",
"Jewelry",
"Counselor",
"Catalog",
"Dialog",
"Analog",
"Monolog",
"Pedagog",
"Prolog",
"Demagog",
"Synagog",
"Aging",
"Judgment",
"Acknowledgment",
"Argument",
"Apneic",
"Anemia",
"Anesthesia",
"Archeology",
"Estrogen",
"Pediatric",
"Leukemia",
"Diarrhea",
"Encyclopedia",
"Hemorrhage",
"Paleolithic",
"Maneuver",
"Inquire",
"Inquiry",
"Installment",
"Enrollment",
"Traveler",
"Canceled",
"Fueled",
"Woolen",
"Labeling",
"Jewelry",
"Plow",
"Smolder",
]
phrase_pattern = re.compile(r'\b(?:' + '|'.join(re.escape(phrase) for phrase in search_phrases) + r')\b', re.IGNORECASE)

# Function to crawl a page and search for specific words
def crawl_page(url):
    global page_count
    if url in visited_urls or not url.startswith(base_url) or page_count >= max_pages:
        return
    
    try:
        response = requests.get(url, headers={'User-Agent': user_agent})
        if "text/html" not in response.headers["Content-Type"]:
            return  # Only process HTML pages
        
        # Increment page count and check if max limit is reached
        page_count += 1
        print(f"Crawling ({page_count}/{max_pages}): {url}")
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Search for the phrases in the page content
        text_content = soup.get_text()
        found_phrases = phrase_pattern.findall(text_content)
        if found_phrases:
            # If phrases are found, write them to the CSV
            with open("found_phrases.csv", mode="a", newline="", encoding="utf-8") as file:
                writer = csv.writer(file)
                writer.writerow([url, ", ".join(set(found_phrases))])
                
        visited_urls.add(url)
        
        # Find and crawl internal links
        for link in soup.find_all("a", href=True):
            href = urljoin(url, link["href"])
            # Skip links with `#` or `?` and ensure only new pages in the `/en-gb` directory are crawled
            if base_url in href and href not in visited_urls and '#' not in href and '?' not in href:
                crawl_page(href)
                
    except requests.RequestException as e:
        print(f"Request failed for {url}: {e}")

# Initialize CSV with headers
with open("found_phrases.csv", mode="w", newline="", encoding="utf-8") as file:
    writer = csv.writer(file)
    writer.writerow(["URL", "Found Phrases"])

# Start crawling from the base URL
crawl_page(base_url)
print("Crawling completed. Check found_phrases.csv for results.")
